{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokens count - GPT_3 (chat-bot example)",
      "provenance": [],
      "authorship_tag": "ABX9TyOlhgdilzbBerdhDXjI3b5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Magda5656/Kodolamacz/blob/master/Tokens_count_GPT_3_(chat_bot_example).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFTvZwwvQTYU"
      },
      "source": [
        "# Simple chat-bot example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXSrPga0Czmt"
      },
      "source": [
        "import os\r\n",
        "import openai\r\n",
        "\r\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\r\n",
        "\r\n",
        "start_sequence = \"\\nAI:\"\r\n",
        "restart_sequence = \"\\nHuman: \"\r\n",
        "\r\n",
        "response = openai.Completion.create(\r\n",
        "  engine=\"davinci\",\r\n",
        "  prompt=\"Human: Hey, how are you doing?\\nAI: I'm good! What would you like to chat about?\\nHuman: \",\r\n",
        "  temperature=0.9,\r\n",
        "  max_tokens=512,\r\n",
        "  top_p=1,\r\n",
        "  frequency_penalty=1,\r\n",
        "  presence_penalty=1,\r\n",
        "  stop=[\"\\nHuman:\", \"\\n\"]\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR6tQlQqSZS8"
      },
      "source": [
        "Counting tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NezcPrwTWRtp"
      },
      "source": [
        "from transformers import GPT2TokenizerFast\r\n",
        "from typing import List\r\n",
        "\r\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\r\n",
        "\r\n",
        "\r\n",
        "def completions_create_tokens(prompt: str, max_tokens: int, n: int = 1) -> int:\r\n",
        "  \"\"\"\r\n",
        "  Returns the upper bound of total tokens consumed for a completions.create() request.\r\n",
        "\r\n",
        "  The formula is\r\n",
        "    \r\n",
        "    prompt_tokens + n * max_tokens\r\n",
        "\r\n",
        "  Note that completions can be shorter than max_tokens if a sample produces a stop sequence. In this case the total tokens are lower than the above estimate.\r\n",
        "  \"\"\"\r\n",
        "  prompt_tokens = tokenizer.encode(prompt)\r\n",
        "  return len(prompt_tokens) + n * max_tokens"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfGSWoNrWXiu"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzMrNnTYWWyk"
      },
      "source": [
        "prompt = \"Human: Hey, how are you doing?\\nAI: I'm good! What would you like to chat about?\\nHuman: \"\r\n",
        "max_tokens=512\r\n",
        "n=2"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77cQTH7DWkYq",
        "outputId": "03c1f66f-8615-46c3-ef27-e9f864a0ec07"
      },
      "source": [
        "completions_create_tokens(prompt,max_tokens,n)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1052"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}